---
title: "R Notes for Multivariate Analysis"
author: "Yongfu, Liao"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: "apalike"
link-citations: yes
description: "This is a note for multivariate analysis in R."
github-repo: "liao961120/MVA.github.io"
favicon: "favicon.ico"
apple-touch-icon: "icon-152x152.png"
apple-touch-icon-size: 152
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	dev = 'svg',
	fig.dim = c(5, 3),
	fig.height = 3, 
	comment = ""
)
```

# About {-}

[![Build Status](https://travis-ci.org/liao961120/MVA.github.io.svg?branch=master)](https://travis-ci.org/liao961120/MVA.github.io){target="_blank"}

This is a very simplified book about Multivariate Analysis in R. It is written as a note to facilitate my learning of [Multivariate Analysis at NTU](https://nol2.aca.ntu.edu.tw/nol/coursesearch/print_table.php?course_id=741%20U3520&class=&dpt_code=7410&ser_no=31954&semester=106-2&lang=CH){target="_blank"}, Spring, 2018.

Feel free to edit (by clicking <i class="fa fa-edit"></i> on the nav bar) if you see any error. Also, forking, modifying, and redistributing is welcomed. For more information about improving this work, see the License section below.


## Acknowledgement

I thank [Aaron Schlegel](https://github.com/aschleg) for letting me modify [his post](https://aaronschlegel.me/factor-analysis-principal-component-method-r.html) on factor analysis with the principle componet method.  
[Chapter 3](FA.html) and [Appendix A](principle-component-method-explained-in-r.html) of this book is largely based on his post.  

<!-- CC Licenese -->
## License {-}

<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" href="http://purl.org/dc/dcmitype/Text" property="dct:title" rel="dct:type">R Notes for Multivariate Analysis</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://liao961120.github.io/MVA.github.io" property="cc:attributionName" rel="cc:attributionURL">Yongfu, Liao</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/">Creative Commons Attribution-NonCommercial 4.0 International License</a>.<br />Based on a work at <a xmlns:dct="http://purl.org/dc/terms/" href="https://github.com/liao961120/MVA.github.io" rel="dct:source">https://github.com/liao961120/MVA.github.io</a>.

<!--chapter:end:index.Rmd-->

# Multivariate Normal Distribution & Covariance Matrix {#mvnorm}


```{r}
library(dplyr)
library(latex2exp)
library(ggplot2)
theme <- theme(axis.text.x = element_text(size = 7, face = "plain", angle = 30),
               axis.text.y = element_text(size = 7, face = "plain"),
               axis.title.x = element_text(size = 9, face = "bold"),
    axis.title.y = element_text(size = 9, face = "bold"))
```


Bivariate Normal Contour Map
-----------------------------------

### `ellipse()`

`ellipse()` from ellipse [@R-ellipse] is used to generate ellipse data based on a correlation/covariance matrix.

```r
ellipse(x, scale, centre, level, npoints = 1000)
```
- `x`: a single number, correlation of the two variables.

- `scale`: vector, **standard deviation** of the two variables.

- `centre`: vector, center of the ellipse, i.e. the mean vector of the bivariate normal distribution.

- `level`: a single number, the contour probability.

- `npoints`: number of points used to draw the contour.

`ellipse` returns a **matrix** with dim(`npoints` $\times$ 2), which can be used to plot contour.

### Data Generation
The `for` loop below is used to generate a data frame with 3 columns(variables):

- Column 1: First variable of bivariate normal function ($x_1$)

- Column 2: Second variable of bivariate normal function ($x_2$)

- Column 3: The contour that $x_1$ & $x_2$ on the same row belongs to.

```{r}
library(ellipse)

All_contours <- c(NA, NA, NA) 
    ## Set empty start for appending ##

for (i in 1:5) {
    level <- 0.1*i 
        ## Set Contour prob., prob. of obs within contour ##
    ell_data <-ellipse(-0.8, c(sqrt(2), 1), centre = c(1, 3), level = level, npoints = 800+(i-1)^3)
        ## npoints: bigger contours with more points ##
    class <- rep(paste(level*100, "% Contour", sep=""), nrow(ell_data))
        ## Assign contour class ##
    ell_data <- as.data.frame(ell_data)
        ## Change to data.frame BEFORE cbind, ##
        ## or coersion happens ##
    ell_data <- cbind(ell_data, class)
    
    All_contours <- rbind(All_contours, ell_data)
}

All_contours <- All_contours[-1,]
    ## Remove the empty start ##
```

### Plotting
```{r}
ggplot(data = All_contours) +
    geom_point(aes(x = x, y = y, color = class),
               size = 0.1) +
    scale_colour_grey(start = 0.7, end = 0.3) +
        ## Use gray scales instead of colored default ##
    labs(color = "Contours", 
         title = "Contour Plot",
         x = TeX("$x_1$"), y = TeX("$x_2$")
    )
```

Multivariate Normal Functions
----------------------------------

### Generate density f(x)

```{r echo=TRUE}
library(mvtnorm)

mu <- c(1, 3) # mean vector
Sigma <- matrix(c(2, -0.8*sqrt(2), -0.8*sqrt(2), 1),
                nrow = 2) # covariance matrix

dmvnorm(x = c(2, 5), mean = mu, sigma = Sigma)
```

- `x`: Vector x in f(x), all variables of the multivariate normal distribution.
- `mean`: Mean vector(center of ellipse) of the multivariate normal distribution.
- `sigma`: Covariance matrix of the multivariate normal distribution.

`dmvnorm` returns f(x), the range of the multivariate normal function. For example, `dmvnorm(x = c(2, 5), mean = mu, sigma = Sigma)` returns the value f($x_1=2$, $x_2=5$) of the multivariate normal distribution specified by mean vector, `mu`, and covariance matrix, `Sigma`.

#### Example: Densities of a Contour 
```{r echo=TRUE}
data <- All_contours %>% 
    filter(class == "50% Contour")

dmvnorm(x = data[1, 1:2], mean = mu, sigma = Sigma)[[1]]
dmvnorm(x = data[4, 1:2], mean = mu, sigma = Sigma)[[1]]
```

The retured values are the same(very close), since they are on the same contour. See the section [above]{#data-generation} for more details.

### Covariance Matrix

Generater covariance and correlation Matricies:
```{r echo=TRUE}
# Function to check whether package is installed
is.installed <- function(mypkg){
    is.element(mypkg, installed.packages()[,1])
} 

# check if package "hydroGOF" is installed
if (!is.installed("mat2tex")){
    library(devtools)
    install_github("markheckmann/mat2tex")
}

library(mat2tex)
cov.mt <- cov(iris[,1:4]) ## Cov Matrix of variable 1~4
cor.mt <- cor(iris[,1:4]) ## Cor Matrix of variable 1~4
```

Covariance matrix 
```{r echo=FALSE, results='asis'}
"$=" %_% xm(cov.mt,2) %_%"$"
```

Correlation matrix 
```{r echo=FALSE, results='asis'}
"$=" %_% xm(cor.mt,2) %_%"$"
```



<!--chapter:end:Multivar_distr.Rmd-->

# Principle Component Analysis {#PCA}

Workflow of PCA
--------------------------------

### Conceptual

```{r echo=FALSE}
library(DiagrammeR)
grViz("
digraph { //non-directed graph
      graph [layout = dot]  //layout = dot|neato|twopi|circo|fdp
      rankdir=TB    // TB|RL|LR, LR: Left to Right orientation

      node [shape = box,
      style = filled,  margin=0.3]
      
      node [fillcolor = gray]
      data [label = 'Data']
      
      node [fillcolor = orange1]  //orange1-4
      cov [label = 'Cov Matrix']
      eigen [label = 'Eigenvalue & Eigenvector']
      PC [label = 'Principle Components' fillcolor = OrangeRed]
      
      edge [color = grey3, arrowhead = normal, arrowtail = none]
      cov -> eigen [label='Compute']
      
      edge [color = grey, arrowhead = normal, arrowtail = none]
      data -> cov [label='Compute']
      data -> eigen [label='Input']
      eigen -> PC [label='Rotate']
}

      ")
```

### Computational (with R)
```{r echo=FALSE}
grViz("
digraph { //non-directed graph
      graph [layout = dot]  //layout = dot|neato|twopi|circo|fdp
      rankdir=TB    // TB|RL|LR, LR: Left to Right orientation

      node [shape = box,
      style = filled,  margin=0.18]
      
      node [fillcolor = gray]
      data [label = 'Data']
      PC [label = 'Principle Components' fillcolor = OrangeRed]
      
      node [fillcolor = WhiteSmoke, shape = oval]  //orange1-4
      sdev [label = 'sdev: std of PCs']
      rotation [label = 'rotation: Loading of PCs on origin vars']
      x [label = 'x: Rotated Data']

      node [fillcolor = Gold, shape = box]
      Scree [label = 'Scree Plot']
      Interpret [label = 'Interpret grouping of vars']
      QQplot [label = 'Q-Q Plot: Single PC']
      scatter [label = 'Scatter Plot of 2 PCs']
      
      edge [color = Darkgray, arrowhead = normal, arrowtail = none]
      data -> PC [label='prcomp()']
      sdev -> Scree [label = 'factoextra::fviz_eig()']
      rotation -> Interpret
      x -> {QQplot scatter}

      edge [style=dashed, color = grey, arrowhead = none, arrowtail = none]
      PC -> {sdev rotation x}
}

      ")
```

- Note: `sdev` of `prcomp()` are **Standard Deviations**. To get the eigenvalues of the covariance (correlation) matrix, or  equivalently, variances of the principle components, you need to **square `sdev`**.


Conversion Between Correlation & Covaraince Matrices
------------------------------------------------------

### `prcomp()`
The function `prcomp()` in base R `stats` package performs principle component analysis to input `data.frame`(with observations as rows and variables as columns), but it returns neither covariance nor correlation matrix. You can compute them directly by passing `data.frame` to `cor()` and `cov()` directly in R without any additional package.

```{block2, type="bs-callout bs-callout-warning"}
[prcomp() vs. princomp()]{.callout-warning-head}

There is another function, `princomp()`, in `stats` that performs PCA. This function is based on spectral decomposition[^spectral] while `prcomp()` is based on SVD[^pca_svd]. SVD has greater numeric accuracy, so `prcomp()` is preferred.
```

### Covariance to Correlation

Sometimes there is no raw data but only covariance or correlation matrix, and you may want to convert one to another. This can be done by using simple matrix multiplication, based on the fact that

$$\mathbf{R} = diag(\mathbf{S})^{\frac{-1}{2}} ~ \mathbf{S} ~ diag(\mathbf{S})^{\frac{-1}{2}}
(\#eq:cov2cor)$$

, where $\mathbf{R}$ is the correlation matrix,  $\mathbf{S}$ is the covariance matrix, and $diag(\mathbf{S})$ is the diagonal matrix composed of  diagonal elements of $\mathbf{S}$.

### `eigen()`

After obtaining the covariance or correlation matrix, direct computation of eigenvalue and eigenvectors is straightforward: pass the matrix to base R `eigen()` function.
```{r}
cov(iris[,1:3]) %>% eigen()
```

Scree Plot
-------------------------------------

Scree plot is an important tool for determining the importance of principle components. Although the logic of plotting scree plots is easy, it may be quite annoying for repeating the code every time.


### `screeplot()` from Base R

There is a ready-written function for scree plot in `stats` package, but the output is terrible:
```{r}
prcomp(iris[,1:3]) %>% screeplot(type="lines")
```


### `fviz_eig()` from `factoextra`

For a better-looking scree plot function, I recommend `fviz_eig()` from `factoextra` [@R-factoextra]. `fviz_eig()` has better looking outputs and more customizable plotting parameters, and since it is based on `ggplot2`, you can actually enhance it with the `ggplot2` syntax: `+`.
```{r}
library(factoextra)
prcomp(iris[,1:3]) %>% fviz_eig()
```

```{r}
prcomp(iris[,1:3]) %>% 
    fviz_eig(choice = "eigenvalue", # y as eigenvalue
             geom = "line",
             addlabels = T) +
    scale_y_continuous(limits = c(0, 5))
```

### Customized Function

I have OCD with plotting, so not completely satisfied with `factoextra::fviz_eig()`. So I created my own `scree_plot()` by building on `fviz_eig()`[^scree], which  supports **double y-axis**: one showing eigenvalue, the other proportion of total variance explained. 


Q-Q Plot
-----------------------------------

Q-Q plots are for checking the normality assumuption and are also useful for detecting outlyers. Principle components are linear combinations of the original variables, so if the original variables come from a multivariate normal distribution, principle components are expected to have normal distributions.


### `qqnorm()` from Base R

There is also a base R `qqnorm()` function, which plots sample quantiles against theoretical quantiles obtain from the standard normal distribution.
```{r}
prcomp(iris[1:60, 1:3])[["x"]][,1] %>% 
    qqnorm()
```

`prcomp(data.frame)[["x"]]` returns the principle component scores, i.e. data that are **rotated** or **weighted** by the elements of the eigenvectors.

`prcomp(data.frame)[["x"]][,1]` subsets the first column of the principle component scores, which is the scores of the First principle component, i.e. data weighted according to elements of the first (corresponding to the largest eigenvalue) eigenvector. 

### Self-defined Function
`qqnorm()` is pretty good but lacks one important feature: **labeling points on the Q-Q plot so that identification of the points is possible**.

So I wrote my own function `QQplot`, which labels every point on the graph:
```{r}
QQplot <- function(x, ID="none", 
                   theme=NULL, color="red", text=TRUE,
                   text_adj=
                       c(hjust=-0.1, vjust=0, size=3)) {
    library(dplyr)
    library(ggplot2)
    
    x <- as_data_frame(x) 
    n <- nrow(x)
    quantiles <- qnorm(p=seq(0.5/n, 1-0.5/n, 1/n))
    
    if (ID == "none") { # assign ID if not passed
        ID <- as.character(1:n)
    } else {
        ID <- as_data_frame(ID)
        ID <- as.character(ID[[colnames(ID)]])
        }
    
    if (text == TRUE) {
        text <- geom_text(aes(label=ID),
                              hjust=text_adj[1],
                              vjust=text_adj[2],
                              size = text_adj[3])
    } else {text <- NULL}
    
    data <- cbind(ID, x)
    colnames(data) <- c("ID", "x")
    data <- data %>% arrange(x) %>% mutate(quantile=quantiles)
    
    pl <- ggplot(data, aes(x=quantiles, y=x))+
        geom_point(color=color)+
        text + theme +
        labs(x="Theoretical Quantile",
             y="x",
             title="Q-Q Plot")
    pl
}
```

```{r}
prcomp(iris[1:60, 1:3])[["x"]][,1] %>%
    QQplot()
```






<!-- FootNotes -->
[^scree]: Check [`multivariate_fc.R`](https://github.com/liao961120/local_depend/blob/master/R%20functions/multivariate_fc.R){target="_blank"}, starting at line 46.

[^pca_svd]: You can check the answers at [Stack Overflow](https://stats.stackexchange.com/questions/20101/what-is-the-difference-between-r-functions-prcomp-and-princomp){target="_blank"}

[^spectral]: The Conceptual workflow of PCA at Section [2.1.1](#conceptual) is based on spectral decomposition.

<!--chapter:end:PCA.Rmd-->

# Factor Analysis {#FA}

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
library(dplyr)
library(knitr)
```

Factor analysis is a technique that represents the variables of a dataset $X_1, X_2, \cdots, X_p$ (or $\mathbf{X}_{p \times 1}$) as linearly related to some fewer unobservable variables called factors, denoted $F_1, F_2, \cdots, F_m$ (or $\mathbf{F}_{m \times 1}$). The factors are representative of **latent variables** underlying the original variables, which is hypothetical as they cannot be measured or observed.


```{block, type="alert alert-success"}
**Notations**:

- Capital Letter: Random Variable
- Lower case Letter: Observation of a random variable
- Bold: Vector or Matrix
- Normal: Single Value
```

The Concept of Factor Analysis
----------------------

The **Orthogonal Factor Model** posits that each variable, $X_i$ is a combination of the underlying latent variables, $F_1, F_2, \cdots, F_m$. For the variables in any of the observation vectors in a sample, the model is defined as:

$$
X_1 - \mu_1 = \ell_{11} F_1 + \ell_{12} F_2 + \cdots + \ell_{1m} F_m + \varepsilon_1 \\
X_2 - \mu_2 = \ell_{21} F_1 + \ell_{22} F_2 + \cdots + \ell_{2m} F_m + \varepsilon_2 \\
\vdots \\
X_p - \mu_p = \ell_{p1} F_1 + \ell_{p2} F_2 + \cdots + \ell_{pm} F_m + \varepsilon_p
(\#eq:model)
$$

, or

$$
\mathbf{X}_{p \times 1} - \mathbf{\mu}_{p \times 1} = \mathbf{L}_{p \times m} \mathbf{F}_{m \times 1} + \mathbf{\varepsilon}_{p \times 1}
(\#eq:model_vec)
$$

, where $\mathbf{\mu}$ is the mean vector and $\mathbf{\varepsilon}$ is a random error term to show the relationship between the factors is not exact. 
There are several assumptions that must be made regarding the relationships of the factor model described above.

1. **Factors are independent** of each other.
    - $E(F_j) = 0$, $var(F_j) = 1$ 
    - $Cov(F_j, F_k) = 0$ where $j \neq k$

2. The error terms $\varepsilon_i$ are **independent** of each other.
    - $E(\varepsilon) = 0$, $var(\varepsilon_i) = \psi_i$
    - $Cov(\varepsilon_i, \varepsilon_j) = 0$.

3. $\varepsilon_i$ and $F_j$ are **independent**: $Cov(\varepsilon_i, F_j) = 0$.

Note the assumption $Cov(\varepsilon_i, \varepsilon_j) = 0$ implies the
**factors represent all correlations** among the variables  $X_i$'s.


### Factoring the Covariance Matrix

With the assumptions above,

$$
\begin{equation} 
\begin{split}
& \mathbf{\Sigma}_{p \times p} = Cov(\mathbf{X}_{p \times p}) = \mathbf{L L^T}_{p \times p} + \mathbf{\Psi}_{p \times p} \\ \\
 , where ~~ & \mathbf{L}_{p \times m} = 
\begin{pmatrix} 
\ell_{11} & \ell_{12} & \cdots & \ell_{1m} \\
\vdots    & \vdots    & \ddots & \vdots    \\
\ell_{p1} & \ell_{p2} & \cdots & \ell_{pm}
\end{pmatrix}, \\
& \mathbf{\Psi}_{p \times p} = 
\begin{pmatrix}
\psi_{1} &        & 0\\
         & \ddots & \\
0        &        & \psi_{p}
\end{pmatrix}
\end{split}
(\#eq:covmt)
\end{equation} 
$$
  
<br><br>

- $Var(X_i) = \sigma_{ii} = \ell^2_{i1} + \ell^2_{i2} + \cdots + \ell^2_{im} + \psi_i$

- $Cov(X_i, X_j) = \sigma_{ij} = \ell_{i1}\ell_{j1} + \ell_{i2}\ell_{j2} + \cdots + \ell_{im}\ell_{jm}$

- $Cov(\mathbf{X}, \mathbf{F}) = \mathbf{L}$, i.e. $\ell_{ij} = Cov(X_i, F_j)$

We therefore have a **partitioning of the variance** of the observation vector $X_i$ into a component due to the common factors $h_i^2$ and a component due to  **specific variance**:

$$
\begin{equation} 
\begin{split}
Var(X_i) & = (\ell^2_{i1} + \ell^2_{i2} + \cdots + \ell^2_{im}) + \psi_i \\
& = h^2_i + \psi_i
\end{split}
(\#eq:varx)
\end{equation}
$$


Principal Component Method
----------------------------------------------

There are several methods for estimating the factor loadings and
communalities, including the **principal component method**, **principal factor method**, the **iterated principal factor method** and **maximum likelihood method**.

The approach of the principal component method is to calculate the **sample covariance matrix** $\mathbf{S}$ from a sample of data and then find an estimator, denoted $\hat{\ell}$ that can be used to factor $\mathbf{S}$.

By **orthogonal diagonalizing** $\mathbf{S}$:

$$
\begin{equation} 
\begin{split}
&\mathbf{S} = PDP^T  \\ \\
, ~ where \ & D =
\begin{pmatrix}
\lambda_1 & & \\
& \ddots & \\
& & \lambda_p
\end{pmatrix}_{p \times p} , \\
& P =
\begin{bmatrix}
e_1 \ \vdots & \cdots & \vdots \ e_p 
\end{bmatrix}_{p \times p}
\end{split}
(\#eq:diagonalize)
\end{equation} 
$$

<br>

$D$ is a **diagonal matrix** with the diagonal entries, $\lambda_1 > \lambda_2 > \cdots > \lambda_p$ equaling the eigenvalues of $\mathbf{S}$.  
$P$ is an **orthogonal matrix** with columns of eigenvectors of $\mathbf{S}$ corresponding to $D$.

By factoring the diagonal matrix $D$, $~D = D^{1/2} ~ D^{1/2}$, since all $\lambda_i \geq 0$:

$$
\begin{equation} 
\begin{split}
\mathbf{S} &= PDP^T = (P D^{1/2})(D^{1/2} P^T) \\
& = (PD^{1/2})(PD^{1/2})^T \\
& = \mathbf{LL^T} 
\end{split}
(\#eq:pcfactor)
\end{equation} 
$$
  
Since we are interested in finding $m$($<p$) factors in the data, we want to find $\mathbf{L}_{p \times m}$.  
By dropping the last $(p-m)$ terms in $D$[^drop], the loading matrix then becomes $\mathbf{L}_{p \times m} = P_{p \times m} ~ D_{m \times m}$. And,  

$$
\begin{equation} 
\begin{split}
& \mathbf{S} \approx \mathbf{L}_{p \times m} \mathbf{L}^T_{m \times p} + \mathbf{\Psi}_{p \times p} \\ \\
, where ~~ & \mathbf{\Psi} = diag(\mathbf{S} - \mathbf{LL}^T) \\ 
& ~~~ = \begin{pmatrix}
s_{11} - h_1^2 & & 0 \\
& \ddots & \\
0 & & s_{pp} - h_p^2
\end{pmatrix}
\end{split}
(\#eq:est-S-with-PC)
\end{equation} 
$$

The number $m$ could be determined by a scree plot.


[^drop]: It's reasonable since the last few $\lambda_i$'s have  small values, hence dropping them doesn't largely affects the total variance of $\mathbf{S}$. 




Factor Analysis with the `psych` Package
----------------------------------------

The [psych package](https://cran.r-project.org/web/packages/psych/) has
many functions available for performing factor analysis.

``` {.r}
library(psych)
```

The `principal()` function performs factor analysis with the principal
component method as explained above. The rotation is set to `none` for
now as we have not yet done any rotation of the factors. The `covar`
argument is set to `TRUE` so the function factors the covariance matrix
$\mathbf{S}$ of the data as we did above.

```{r}
library(psych)
iris.FA.PC <- 
    principal(iris[,-5], 
              nfactors = 2, 
              rotate = 'none', 
              covar = TRUE)
iris.FA.PC
```

The function's output matches our calculations. **H2** and **U2** are the ***communality** and **specific variance**, respectively, of the **standardized loadings** obtained from the **correlation matrix** $\mathbf{R}$. 

```{r}
iris.FA.PC[["communality"]]
```


```{r}
iris.FA.PC[["loadings"]]
```


```{r}
iris.FA.PC[["Vaccounted"]]
```
`Proportion Var` is the proportion of total variance due to the column of $\hat{\mathbf{L}}$, i.e the common factor.

`Proportion Explained` is the variance due to one common factor devided by variance due to all common factors.




<!--chapter:end:FA.Rmd-->

# (APPENDIX) Appendix {-} 

<!--chapter:end:appendix.Rmd-->

# Principle Component Method Explained (in R)

```{r echo=FALSE, message=FALSE, warning=FALSE, paged.print=FALSE}
library(dplyr)
library(knitr)
library(kableExtra)
```

The following example demonstrates factor analysis using the **covariance matrix** with the `iris` data set in R. Instead of directly using `psych::priciple()`, a step-by-step approach is used.

```{r echo=FALSE, message=FALSE, warning=FALSE}
head(iris) %>% 
    kable(format="html", align="c") %>%
    kable_styling(bootstrap_options = 
                      c("striped", "condensed")) %>%
    scroll_box(width = "100%")
```

## $\lambda_i$ & $e_i$ of $\mathbf{S}$

Find the covariance matrix $\mathbf{S}$ with the `cov()` function.

```{r message=FALSE, warning=FALSE}
S <- cov(iris[,-5])
S  %>% 
    kable(format="html", align="c", digits = 3) %>%
    kable_styling(bootstrap_options = 
                      c("striped", "condensed")) %>%
    scroll_box(width = "100%")
```

The eigenvalues and eigenvectors are then computed from the covariance matrix with the `eigen()` function.

```{r}
S.eigen <- eigen(S)
S.eigen
```

## Construction of $\hat{\mathbf{L}}$

Before proceeding with factoring $\mathbf{S}$ into $PDP^T$, the number of factors $m$ must be selected. The last two eigenvalues of $\mathbf{S}$ are practically
$0$, so $m = 2$ is likely a good choice. 


With $m = 2$ factors, construct the $P$ and $D$ matrices from the covariance matrix with the largest two eigenvalues and the corresponding eigenvectors.

```{r}
P <- S.eigen$vectors[,1:2] %>% as.matrix()

D <- diag(S.eigen$values[1:2], 
          nrow = dim(P)[2],
          ncol = dim(P)[2])
```

$\hat{\mathbf{L}}$ is then found from the $P$ and $D$ matrices as in $\hat{\mathbf{L}} = PD^{1/2}$

```{r}
S.loadings <- P %*% sqrt(D)
S.loadings %>% round(3)
```

Which are the **unrotated factor loadings**. We can see where the term **principal component method** is derived from as the columns of $\hat{\mathbf{L}}$ are proportional to the eigenvectors of $\mathbf{S}$, which are also equal to the corresponding coefficient of the principal components.

```{r}
# Perform PCA and take the resulting first two PCs
prcomp(iris[,-5])$rotation[,1:2] %>% 
    round(3)
```

```{r}
# eigenvector of S from iris
S.eigen$vectors[,1:2] %>% 
    round(3)
```

## Variances: Communalities, & Specific Variances

The **communality** $h^2_i$, as noted previously is the sum of squares of the **row** of $\hat{\mathbf{L}}$.

$$ \hat{h}^2_i = \sum^m_{j=1} \hat{\ell}^2_{ij} $$

```{r}
S.h2 <- S.loadings^2 %>% rowSums()
S.h2 %>% round(3)
```

The sum of squares of the columns of $\hat{\mathbf{L}}$ are the respective eigenvalues of $\mathbf{S}$,
since $\sqrt{\hat{\lambda_i}} \hat{e_i} = 
\begin{pmatrix}
\ell_{i1} \\
\ell_{i2} \\
\vdots \\
\ell_{ip} \\
\end{pmatrix}_{p \times 1}$,
$(\sqrt{\hat{\lambda_i}} \hat{e_i})^T (\sqrt{\hat{\lambda_i}} \hat{e_i}) = \hat{\lambda_i}$

```{r}
colSums(S.loadings^2) %>% round(3)
```

```{r}
S.eigen$values[1:2] %>% round(3)
```

### Specific variance $\psi_i$

The specific variance, $\psi_i$, is a component unique to the particular variable and is found by subtracting the diagonal of $\mathbf{S}$ by the respective communality $\hat{h}^2_i$:

$$ \psi_i = s_{ii} - \hat{h}^2_i $$

```{r}
diag(S) - S.h2 %>% 
    round(4)
```

### Proportion of variance due to Factors

The proportions of **total variance** of $\mathbf{S}$ due to **common factor** 1 and 2, respectively, are found by dividing the sum of squares of the columns of $\hat{\mathbf{L}}$ by $tr(\mathbf{S}) = \Sigma s_{ii} = \Sigma \lambda_i$.

```{r}
var_F1 <- colSums(S.loadings^2)[1]
var_F2 <- colSums(S.loadings^2)[2]

cbind(var_F1 / sum(S.eigen$values),
      var_F2 / sum(S.eigen$values)) %>%
    round(3)
```

<!--chapter:end:ap_FA_PCmethod.Rmd-->

```{r include=FALSE}
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

---
nocite: |
  @*
...

`r if (knitr::is_html_output()) '# Package Used {#references -}'`




<!--chapter:end:references.Rmd-->

