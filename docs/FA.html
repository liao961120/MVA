<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>R Notes for Multivariate Analysis</title>
  <meta name="description" content="This is a note for multivariate analysis in R.">
  <meta name="generator" content="bookdown 0.6 and GitBook 2.6.7">

  <meta property="og:title" content="R Notes for Multivariate Analysis" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a note for multivariate analysis in R." />
  <meta name="github-repo" content="liao961120/MVA.github.io" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="R Notes for Multivariate Analysis" />
  
  <meta name="twitter:description" content="This is a note for multivariate analysis in R." />
  

<meta name="author" content="Yongfu, Liao">


<meta name="date" content="2018-04-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  <link rel="apple-touch-icon-precomposed" sizes="152x152" href="icon-152x152.png">
  <link rel="shortcut icon" href="favicon.ico" type="image/x-icon">
<link rel="prev" href="PCA.html">
<link rel="next" href="references.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.0/htmlwidgets.js"></script>
<script src="libs/viz-0.3/viz.js"></script>
<link href="libs/DiagrammeR-styles-0.2/styles.css" rel="stylesheet" />
<script src="libs/grViz-binding-0.9.2/grViz.js"></script>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">R Notes for Multivariate Analysis</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a><ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i>License</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="mvnorm.html"><a href="mvnorm.html"><i class="fa fa-check"></i><b>1</b> Multivariate Normal Distribution &amp; Covariance Matrix</a><ul>
<li class="chapter" data-level="1.1" data-path="mvnorm.html"><a href="mvnorm.html#bivariate-normal-contour-map"><i class="fa fa-check"></i><b>1.1</b> Bivariate Normal Contour Map</a><ul>
<li class="chapter" data-level="1.1.1" data-path="mvnorm.html"><a href="mvnorm.html#ellipse"><i class="fa fa-check"></i><b>1.1.1</b> <code>ellipse()</code></a></li>
<li class="chapter" data-level="1.1.2" data-path="mvnorm.html"><a href="mvnorm.html#data-generation"><i class="fa fa-check"></i><b>1.1.2</b> Data Generation</a></li>
<li class="chapter" data-level="1.1.3" data-path="mvnorm.html"><a href="mvnorm.html#plotting"><i class="fa fa-check"></i><b>1.1.3</b> Plotting</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="mvnorm.html"><a href="mvnorm.html#multivariate-normal-functions"><i class="fa fa-check"></i><b>1.2</b> Multivariate Normal Functions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="mvnorm.html"><a href="mvnorm.html#generate-density-fx"><i class="fa fa-check"></i><b>1.2.1</b> Generate density f(x)</a></li>
<li class="chapter" data-level="1.2.2" data-path="mvnorm.html"><a href="mvnorm.html#covariance-matrix"><i class="fa fa-check"></i><b>1.2.2</b> Covariance Matrix</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="PCA.html"><a href="PCA.html"><i class="fa fa-check"></i><b>2</b> Principle Component Analysis</a><ul>
<li class="chapter" data-level="2.1" data-path="PCA.html"><a href="PCA.html#workflow-of-pca"><i class="fa fa-check"></i><b>2.1</b> Workflow of PCA</a><ul>
<li class="chapter" data-level="2.1.1" data-path="PCA.html"><a href="PCA.html#conceptual"><i class="fa fa-check"></i><b>2.1.1</b> Conceptual</a></li>
<li class="chapter" data-level="2.1.2" data-path="PCA.html"><a href="PCA.html#computational-with-r"><i class="fa fa-check"></i><b>2.1.2</b> Computational (with R)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="PCA.html"><a href="PCA.html#conversion-between-correlation-covaraince-matrices"><i class="fa fa-check"></i><b>2.2</b> Conversion Between Correlation &amp; Covaraince Matrices</a><ul>
<li class="chapter" data-level="2.2.1" data-path="PCA.html"><a href="PCA.html#prcomp"><i class="fa fa-check"></i><b>2.2.1</b> <code>prcomp()</code></a></li>
<li class="chapter" data-level="2.2.2" data-path="PCA.html"><a href="PCA.html#covariance-to-correlation"><i class="fa fa-check"></i><b>2.2.2</b> Covariance to Correlation</a></li>
<li class="chapter" data-level="2.2.3" data-path="PCA.html"><a href="PCA.html#eigen"><i class="fa fa-check"></i><b>2.2.3</b> <code>eigen()</code></a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="PCA.html"><a href="PCA.html#scree-plot"><i class="fa fa-check"></i><b>2.3</b> Scree Plot</a><ul>
<li class="chapter" data-level="2.3.1" data-path="PCA.html"><a href="PCA.html#screeplot-from-base-r"><i class="fa fa-check"></i><b>2.3.1</b> <code>screeplot()</code> from Base R</a></li>
<li class="chapter" data-level="2.3.2" data-path="PCA.html"><a href="PCA.html#fviz_eig-from-factoextra"><i class="fa fa-check"></i><b>2.3.2</b> <code>fviz_eig()</code> from <code>factoextra</code></a></li>
<li class="chapter" data-level="2.3.3" data-path="PCA.html"><a href="PCA.html#customized-function"><i class="fa fa-check"></i><b>2.3.3</b> Customized Function</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="PCA.html"><a href="PCA.html#q-q-plot"><i class="fa fa-check"></i><b>2.4</b> Q-Q Plot</a><ul>
<li class="chapter" data-level="2.4.1" data-path="PCA.html"><a href="PCA.html#qqnorm-from-base-r"><i class="fa fa-check"></i><b>2.4.1</b> <code>qqnorm()</code> from Base R</a></li>
<li class="chapter" data-level="2.4.2" data-path="PCA.html"><a href="PCA.html#self-defined-function"><i class="fa fa-check"></i><b>2.4.2</b> Self-defined Function</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="FA.html"><a href="FA.html"><i class="fa fa-check"></i><b>3</b> Factor Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="FA.html"><a href="FA.html#the-concept-of-factor-analysis"><i class="fa fa-check"></i><b>3.1</b> The Concept of Factor Analysis</a><ul>
<li class="chapter" data-level="3.1.1" data-path="FA.html"><a href="FA.html#factoring-the-covariance-matrix"><i class="fa fa-check"></i><b>3.1.1</b> Factoring the Covariance Matrix</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="FA.html"><a href="FA.html#estimation-of-factor-loadings-and-communalities-with-the-principal-component-method"><i class="fa fa-check"></i><b>3.2</b> Estimation of Factor Loadings and Communalities with the Principal Component Method</a></li>
<li class="chapter" data-level="3.3" data-path="FA.html"><a href="FA.html#principal-component-method-of-factor-analysis-in-r"><i class="fa fa-check"></i><b>3.3</b> Principal Component Method of Factor Analysis in R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="FA.html"><a href="FA.html#lambda_i-e_i-of-mathbfs"><i class="fa fa-check"></i><b>3.3.1</b> <span class="math inline">\(\lambda_i\)</span> &amp; <span class="math inline">\(e_i\)</span> of <span class="math inline">\(\mathbf{S}\)</span></a></li>
<li class="chapter" data-level="3.3.2" data-path="FA.html"><a href="FA.html#construction-of-hatmathbfl"><i class="fa fa-check"></i><b>3.3.2</b> Construction of <span class="math inline">\(\hat{\mathbf{L}}\)</span></a></li>
<li class="chapter" data-level="3.3.3" data-path="FA.html"><a href="FA.html#variances-communalities-specific-variances"><i class="fa fa-check"></i><b>3.3.3</b> Variances: Communalities, &amp; Specific Variances</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="FA.html"><a href="FA.html#factor-analysis-with-the-psych-package"><i class="fa fa-check"></i><b>3.4</b> Factor Analysis with the <code>psych</code> Package</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>Package Used</a></li>
<li class="divider"></li>
<li></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R Notes for Multivariate Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="FA" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Factor Analysis</h1>
<p>Factor analysis is a technique that represents the variables of a dataset <span class="math inline">\(X_1, X_2, \cdots, X_p\)</span> (or <span class="math inline">\(\mathbf{X}_{p \times 1}\)</span>) as linearly related to some fewer unobservable variables called factors, denoted <span class="math inline">\(F_1, F_2, \cdots, F_m\)</span> (or <span class="math inline">\(\mathbf{F}_{m \times 1}\)</span>). The factors are representative of <strong>latent variables</strong> underlying the original variables, which is hypothetical as they cannot be measured or observed.</p>
<div class="alert alert-success">
<p>
<strong>Notations</strong>:
</p>
<ul>
<li>
Capital Letter: Random Variable
</li>
<li>
Lower case Letter: Observation of a random variable
</li>
<li>
Bold: Vector or Matrix
</li>
<li>
Normal: Single Value
</li>
</ul>
</div>
<div id="the-concept-of-factor-analysis" class="section level2">
<h2><span class="header-section-number">3.1</span> The Concept of Factor Analysis</h2>
<p>The <strong>Orthogonal Factor Model</strong> posits that each variable, <span class="math inline">\(X_i\)</span> is a combination of the underlying latent variables, <span class="math inline">\(F_1, F_2, \cdots, F_m\)</span>. For the variables in any of the observation vectors in a sample, the model is defined as:</p>
<p><span class="math display" id="eq:model">\[
X_1 - \mu_1 = \ell_{11} F_1 + \ell_{12} F_2 + \cdots + \ell_{1m} F_m + \varepsilon_1 \\
X_2 - \mu_2 = \ell_{21} F_1 + \ell_{22} F_2 + \cdots + \ell_{2m} F_m + \varepsilon_2 \\
\vdots \\
X_p - \mu_p = \ell_{p1} F_1 + \ell_{p2} F_2 + \cdots + \ell_{pm} F_m + \varepsilon_p
\tag{3.1}
\]</span></p>
<p>, or</p>
<p><span class="math display">\[
\mathbf{X}_{p \times 1} - \mathbf{\mu}_{p \times 1} = \mathbf{L}_{p \times m} \mathbf{F}_{m \times 1} + \mathbf{\varepsilon}_{p \times 1}
(\#eq:model_vec)
\]</span></p>
<p>Where <span class="math inline">\(\mathbf{\mu}\)</span> is the mean vector and <span class="math inline">\(\mathbf{\varepsilon}\)</span> is a random error term to show the relationship between the factors is not exact. There are several assumptions that must be made regarding the relationships of the factor model described above.</p>
<ol style="list-style-type: decimal">
<li><strong>Factors are independent</strong> of each other.
<ul>
<li><span class="math inline">\(E(F_j) = 0\)</span>, <span class="math inline">\(var(F_j) = 1\)</span></li>
<li><span class="math inline">\(Cov(F_j, F_k) = 0\)</span> where <span class="math inline">\(j \neq k\)</span></li>
</ul></li>
<li>The error terms <span class="math inline">\(\varepsilon_i\)</span> are <strong>independent</strong> of each other.
<ul>
<li><span class="math inline">\(E(\varepsilon) = 0\)</span>, <span class="math inline">\(var(\varepsilon_i) = \psi_i\)</span></li>
<li><span class="math inline">\(Cov(\varepsilon_i, \varepsilon_j) = 0\)</span>.</li>
</ul></li>
<li><span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(F_j\)</span> are <strong>independent</strong>: <span class="math inline">\(Cov(\varepsilon_i, F_j) = 0\)</span>.</li>
</ol>
<p>Note the assumption <span class="math inline">\(Cov(\varepsilon_i, \varepsilon_j) = 0\)</span> implies the <strong>factors represent all correlations</strong> among the variables <span class="math inline">\(X_i\)</span>’s.</p>
<div id="factoring-the-covariance-matrix" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Factoring the Covariance Matrix</h3>
<p>With the assumptions above,</p>
<span class="math display" id="eq:covmt">\[
\mathbf{\Sigma}_{p \times p} = Cov(\mathbf{X}_{p \times p}) = \mathbf{L L^T}_{p \times p} + \mathbf{\Psi}_{p \times p}
\tag{3.2}
\]</span> , where $ ~ _{p m} =
<span class="math display">\[\begin{pmatrix} 
\ell_{11} &amp; \ell_{12} &amp; \cdots &amp; \ell_{1m} \\
\vdots    &amp; \vdots    &amp; \ddots &amp; \vdots    \\
\ell_{p1} &amp; \ell_{p2} &amp; \cdots &amp; \ell_{pm}
\end{pmatrix}\]</span>
, ~ _{p p} =
<span class="math display">\[\begin{pmatrix}
\psi_{1} &amp;        &amp; 0\\
         &amp; \ddots &amp; \\
0        &amp;        &amp; \psi_{p}
\end{pmatrix}\]</span>
<p>$</p>
<p><br><br></p>
<ul>
<li><p><span class="math inline">\(Var(X_i) = \sigma_{ii} = \ell^2_{i1} + \ell^2_{i2} + \cdots + \ell^2_{im} + \psi_i\)</span></p></li>
<li><p><span class="math inline">\(Cov(X_i, X_j) = \sigma_{ij} = \ell_{i1}\ell_{j1} + \ell_{i2}\ell_{j2} + \cdots + \ell_{im}\ell_{jm}\)</span></p></li>
<li><p><span class="math inline">\(Cov(\mathbf{X}, \mathbf{F}) = \mathbf{L}\)</span>, i.e. <span class="math inline">\(\ell_{ij} = Cov(X_i, F_j)\)</span></p></li>
</ul>
<p>We therefore have a <strong>partitioning of the variance</strong> of the observation vector <span class="math inline">\(X_i\)</span> into a component due to the common factors <span class="math inline">\(h_i^2\)</span> and a component due to <strong>specific variance</strong>:</p>
<p><span class="math display" id="eq:varx">\[
\begin{align}
Var(X_i) &amp;= (\ell^2_{i1} + \ell^2_{i2} + \cdots + \ell^2_{im}) + \psi_i \\
&amp;= h^2_i + \psi_i
\end{align}
\tag{3.3}
\]</span></p>
</div>
</div>
<div id="estimation-of-factor-loadings-and-communalities-with-the-principal-component-method" class="section level2">
<h2><span class="header-section-number">3.2</span> Estimation of Factor Loadings and Communalities with the Principal Component Method</h2>
<p>There are several methods for estimating the factor loadings and communalities, including the <strong>principal component method</strong>, <strong>principal factor method</strong>, the <strong>iterated principal factor method</strong> and <strong>maximum likelihood method</strong>.</p>
<p>The approach of the principal component method is to calculate the <strong>sample covariance matrix</strong> <span class="math inline">\(\mathbf{S}\)</span> from a sample of data and then find an estimator, denoted <span class="math inline">\(\hat{\ell}\)</span> that can be used to factor <span class="math inline">\(\mathbf{S}\)</span>.</p>
<p>By <strong>orthogonal diagonalizing</strong> <span class="math inline">\(\mathbf{S}\)</span>:</p>
<p><span class="math display" id="eq:diagonalize">\[
\mathbf{S} = PDP^T \ ,
\tag{3.4}
\]</span></p>
<p>,where <span class="math inline">\(\ D = \begin{pmatrix} \lambda_1 &amp; &amp; \\ &amp; \ddots &amp; \\ &amp; &amp; \lambda_p \end{pmatrix}_{p \times p} \ , P = \begin{bmatrix} e_1 \ \vdots &amp; \cdots &amp; \vdots \ e_p \end{bmatrix}_{p \times p}\)</span>.</p>
<p><br></p>
<p><span class="math inline">\(D\)</span> is a <strong>diagonal matrix</strong> with the diagonal entries, <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt; \cdots &gt; \lambda_p\)</span> equaling the eigenvalues of <span class="math inline">\(\mathbf{S}\)</span>.<br />
<span class="math inline">\(P\)</span> is an <strong>orthogonal matrix</strong> with columns of eigenvectors of <span class="math inline">\(\mathbf{S}\)</span> corresponding to <span class="math inline">\(D\)</span>.</p>
<p>By factoring the diagonal matrix <span class="math inline">\(D\)</span> into (since all <span class="math inline">\(\lambda_i \geq 0\)</span>):</p>
<p><span class="math display">\[D = D^{1/2} ~ D^{1/2}\]</span></p>
<p><span class="math display" id="eq:pcfactor">\[
\begin{align}
\mathbf{S} &amp;= PDP^T = (P D^{1/2})(D^{1/2} P^T) \\
&amp;= (PD^{1/2})(PD^{1/2})^T \\
&amp;= \mathbf{LL^T} 
\end{align}
\tag{3.5}
\]</span></p>
<p>Since we are interested in finding <span class="math inline">\(m\)</span>(<span class="math inline">\(&lt;p\)</span>) factors in the data, we want to find <span class="math inline">\(\mathbf{L}_{p \times m}\)</span>.<br />
By dropping the last <span class="math inline">\((p-m)\)</span> terms in <span class="math inline">\(D\)</span><a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a>, the loading matrix then becomes <span class="math inline">\(\mathbf{L}_{p \times m} = P_{p \times m} ~ D_{m \times m}\)</span>. And,</p>
<p><span class="math display" id="eq:pcS">\[
\mathbf{S} \approx \mathbf{L}_{p \times m} \mathbf{L}^T_{m \times p} + \mathbf{\Psi}_{p \times p} \\
\tag{3.6}
\]</span></p>
<p>, where <span class="math inline">\(\mathbf{\Psi} = diag(\mathbf{S} - \mathbf{LL}^T) = \begin{pmatrix} s_{11} - h_1^2 &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; s_{pp} - h_p^2 \end{pmatrix}\)</span></p>
<p>The number <span class="math inline">\(m\)</span> could be determined by a scree plot.</p>
</div>
<div id="principal-component-method-of-factor-analysis-in-r" class="section level2">
<h2><span class="header-section-number">3.3</span> Principal Component Method of Factor Analysis in R</h2>
<p>The following example demonstrates factor analysis using the <strong>covariance matrix</strong> with the <code>iris</code> data set in R.</p>
<table>
<thead>
<tr class="header">
<th align="center">Sepal.Length</th>
<th align="center">Sepal.Width</th>
<th align="center">Petal.Length</th>
<th align="center">Petal.Width</th>
<th align="center">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">5.1</td>
<td align="center">3.5</td>
<td align="center">1.4</td>
<td align="center">0.2</td>
<td align="center">setosa</td>
</tr>
<tr class="even">
<td align="center">4.9</td>
<td align="center">3.0</td>
<td align="center">1.4</td>
<td align="center">0.2</td>
<td align="center">setosa</td>
</tr>
<tr class="odd">
<td align="center">4.7</td>
<td align="center">3.2</td>
<td align="center">1.3</td>
<td align="center">0.2</td>
<td align="center">setosa</td>
</tr>
<tr class="even">
<td align="center">4.6</td>
<td align="center">3.1</td>
<td align="center">1.5</td>
<td align="center">0.2</td>
<td align="center">setosa</td>
</tr>
<tr class="odd">
<td align="center">5.0</td>
<td align="center">3.6</td>
<td align="center">1.4</td>
<td align="center">0.2</td>
<td align="center">setosa</td>
</tr>
<tr class="even">
<td align="center">5.4</td>
<td align="center">3.9</td>
<td align="center">1.7</td>
<td align="center">0.4</td>
<td align="center">setosa</td>
</tr>
</tbody>
</table>
<div id="lambda_i-e_i-of-mathbfs" class="section level3">
<h3><span class="header-section-number">3.3.1</span> <span class="math inline">\(\lambda_i\)</span> &amp; <span class="math inline">\(e_i\)</span> of <span class="math inline">\(\mathbf{S}\)</span></h3>
<p>Find the covariance matrix <span class="math inline">\(\mathbf{S}\)</span> with the <code>cov()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S &lt;-<span class="st"> </span><span class="kw">cov</span>(iris[,<span class="op">-</span><span class="dv">5</span>])
<span class="kw">round</span>(S, <span class="dv">3</span>)</code></pre></div>
<pre><code>             Sepal.Length Sepal.Width Petal.Length Petal.Width
Sepal.Length        0.686      -0.042        1.274       0.516
Sepal.Width        -0.042       0.190       -0.330      -0.122
Petal.Length        1.274      -0.330        3.116       1.296
Petal.Width         0.516      -0.122        1.296       0.581</code></pre>
<p>The eigenvalues and eigenvectors are then computed from the covariance matrix with the <code>eigen()</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S.eigen &lt;-<span class="st"> </span><span class="kw">eigen</span>(S)
S.eigen</code></pre></div>
<pre><code>eigen() decomposition
$values
[1] 4.22824171 0.24267075 0.07820950 0.02383509

$vectors
            [,1]        [,2]        [,3]       [,4]
[1,]  0.36138659 -0.65658877 -0.58202985  0.3154872
[2,] -0.08452251 -0.73016143  0.59791083 -0.3197231
[3,]  0.85667061  0.17337266  0.07623608 -0.4798390
[4,]  0.35828920  0.07548102  0.54583143  0.7536574</code></pre>
</div>
<div id="construction-of-hatmathbfl" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Construction of <span class="math inline">\(\hat{\mathbf{L}}\)</span></h3>
<p>Before proceeding with factoring <span class="math inline">\(\mathbf{S}\)</span> into <span class="math inline">\(PDP^T\)</span>, the number of factors <span class="math inline">\(m\)</span> must be selected. The last two eigenvalues of <span class="math inline">\(\mathbf{S}\)</span> are practically <span class="math inline">\(0\)</span>, so <span class="math inline">\(m = 2\)</span> is likely a good choice.</p>
<p>With <span class="math inline">\(m = 2\)</span> factors, construct the <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> matrices from the covariance matrix with the largest two eigenvalues and the corresponding eigenvectors.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">P &lt;-<span class="st"> </span>S.eigen<span class="op">$</span>vectors[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">as.matrix</span>()

D &lt;-<span class="st"> </span><span class="kw">diag</span>(S.eigen<span class="op">$</span>values[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>], 
          <span class="dt">nrow =</span> <span class="kw">dim</span>(P)[<span class="dv">2</span>],
          <span class="dt">ncol =</span> <span class="kw">dim</span>(P)[<span class="dv">2</span>])</code></pre></div>
<p><span class="math inline">\(\hat{\mathbf{L}}\)</span> is then found from the <span class="math inline">\(P\)</span> and <span class="math inline">\(D\)</span> matrices as in <span class="math inline">\(\hat{\mathbf{L}} = PD^{1/2}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S.loadings &lt;-<span class="st"> </span>P <span class="op">%*%</span><span class="st"> </span><span class="kw">sqrt</span>(D)
<span class="kw">round</span>(S.loadings, <span class="dv">3</span>)</code></pre></div>
<pre><code>       [,1]   [,2]
[1,]  0.743 -0.323
[2,] -0.174 -0.360
[3,]  1.762  0.085
[4,]  0.737  0.037</code></pre>
<p>Which are the <strong>unrotated factor loadings</strong>. We can see where the term <strong>principal component method</strong> is derived from as the columns of <span class="math inline">\(\hat{\mathbf{L}}\)</span> are proportional to the eigenvectors of <span class="math inline">\(\mathbf{S}\)</span>, which are also equal to the corresponding coefficient of the principal components.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Perform PCA and take the resulting first two PCs</span>
<span class="kw">prcomp</span>(iris[,<span class="op">-</span><span class="dv">5</span>])<span class="op">$</span>rotation[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]<span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>                PC1    PC2
Sepal.Length  0.361 -0.657
Sepal.Width  -0.085 -0.730
Petal.Length  0.857  0.173
Petal.Width   0.358  0.075</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># eigenvector of S from iris</span>
S.eigen<span class="op">$</span>vectors[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>] <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>       [,1]   [,2]
[1,]  0.361 -0.657
[2,] -0.085 -0.730
[3,]  0.857  0.173
[4,]  0.358  0.075</code></pre>
</div>
<div id="variances-communalities-specific-variances" class="section level3">
<h3><span class="header-section-number">3.3.3</span> Variances: Communalities, &amp; Specific Variances</h3>
<p>The <strong>communality</strong> <span class="math inline">\(h^2_i\)</span>, as noted previously is the sum of squares of the <strong>row</strong> of <span class="math inline">\(\hat{\mathbf{L}}\)</span>.</p>
<p><span class="math display">\[ \hat{h}^2_i = \sum^m_{j=1} \hat{\ell}^2_{ij} \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S.h2 &lt;-<span class="st"> </span>S.loadings<span class="op">^</span><span class="dv">2</span> <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">rowSums</span>()
<span class="kw">round</span>(S.h2, <span class="dv">3</span>)</code></pre></div>
<pre><code>[1] 0.657 0.160 3.110 0.544</code></pre>
<p>The sum of squares of the columns of <span class="math inline">\(\hat{\mathbf{L}}\)</span> are the respective eigenvalues of <span class="math inline">\(\mathbf{S}\)</span>, since <span class="math inline">\(\sqrt{\hat{\lambda_i}} \hat{e_i} = \begin{pmatrix} \ell_{i1} \\ \ell_{i2} \\ \vdots \\ \ell_{ip} \\ \end{pmatrix}_{p \times 1}\)</span>, <span class="math inline">\((\sqrt{\hat{\lambda_i}} \hat{e_i})^T (\sqrt{\hat{\lambda_i}} \hat{e_i}) = \hat{\lambda_i}\)</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">colSums</span>(S.loadings<span class="op">^</span><span class="dv">2</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>[1] 4.228 0.243</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">S.eigen<span class="op">$</span>values[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>] <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>[1] 4.228 0.243</code></pre>
<div id="specific-variance-psi_i" class="section level4">
<h4><span class="header-section-number">3.3.3.1</span> Specific variance <span class="math inline">\(\psi_i\)</span></h4>
<p>The specific variance, <span class="math inline">\(\psi_i\)</span>, is a component unique to the particular variable and is found by subtracting the diagonal of <span class="math inline">\(\mathbf{S}\)</span> by the respective communality <span class="math inline">\(\hat{h}^2_i\)</span>:</p>
<p><span class="math display">\[ \psi_i = s_{ii} - \hat{h}^2_i \]</span></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diag</span>(S) <span class="op">-</span><span class="st"> </span>S.h2 <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">    </span><span class="kw">round</span>(<span class="dv">4</span>)</code></pre></div>
<pre><code>Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
 0.028893512  0.030379418  0.005977852  0.036806264 </code></pre>
</div>
<div id="proportion-of-variance-due-to-factors" class="section level4">
<h4><span class="header-section-number">3.3.3.2</span> Proportion of variance due to Factors</h4>
<p>The proportions of <strong>total variance</strong> of <span class="math inline">\(\mathbf{S}\)</span> due to <strong>common factor</strong> 1 and 2, respectively, are found by dividing the sum of squares of the columns of <span class="math inline">\(\hat{\mathbf{L}}\)</span> by <span class="math inline">\(tr(\mathbf{S}) = \Sigma s_{ii} = \Sigma \lambda_i\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">var_F1 &lt;-<span class="st"> </span><span class="kw">colSums</span>(S.loadings<span class="op">^</span><span class="dv">2</span>)[<span class="dv">1</span>]
var_F2 &lt;-<span class="st"> </span><span class="kw">colSums</span>(S.loadings<span class="op">^</span><span class="dv">2</span>)[<span class="dv">2</span>]

<span class="kw">cbind</span>(var_F1 <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(S.eigen<span class="op">$</span>values),
      var_F2 <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(S.eigen<span class="op">$</span>values)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">round</span>(<span class="dv">3</span>)</code></pre></div>
<pre><code>      [,1]  [,2]
[1,] 0.925 0.053</code></pre>
</div>
</div>
</div>
<div id="factor-analysis-with-the-psych-package" class="section level2">
<h2><span class="header-section-number">3.4</span> Factor Analysis with the <code>psych</code> Package</h2>
<p>The <a href="https://cran.r-project.org/web/packages/psych/">psych package</a> has many functions available for performing factor analysis.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)</code></pre></div>
<p>The <code>principal()</code> function performs factor analysis with the principal component method as explained above. The rotation is set to <code>none</code> for now as we have not yet done any rotation of the factors. The <code>covar</code> argument is set to <code>TRUE</code> so the function factors the covariance matrix <span class="math inline">\(\mathbf{S}\)</span> of the data as we did above.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(psych)
iris.FA.PC &lt;-<span class="st"> </span>
<span class="st">    </span><span class="kw">principal</span>(iris[,<span class="op">-</span><span class="dv">5</span>], 
              <span class="dt">nfactors =</span> <span class="dv">2</span>, 
              <span class="dt">rotate =</span> <span class="st">&#39;none&#39;</span>, 
              <span class="dt">covar =</span> <span class="ot">TRUE</span>)
iris.FA.PC</code></pre></div>
<pre><code>Principal Components Analysis
Call: principal(r = iris[, -5], nfactors = 2, rotate = &quot;none&quot;, covar = TRUE)
Unstandardized loadings (pattern matrix) based upon covariance matrix
               PC1   PC2   h2     u2   H2     U2
Sepal.Length  0.74  0.32 0.66 0.0289 0.96 0.0421
Sepal.Width  -0.17  0.36 0.16 0.0304 0.84 0.1600
Petal.Length  1.76 -0.09 3.11 0.0059 1.00 0.0019
Petal.Width   0.74 -0.04 0.54 0.0368 0.94 0.0634

                       PC1  PC2
SS loadings           4.23 0.24
Proportion Var        0.92 0.05
Cumulative Var        0.92 0.98
Proportion Explained  0.95 0.05
Cumulative Proportion 0.95 1.00

 Standardized loadings (pattern matrix)
             item   PC1   PC2   h2     u2
Sepal.Length    1  0.90  0.39 0.96 0.0421
Sepal.Width     2 -0.40  0.83 0.84 0.1600
Petal.Length    3  1.00 -0.05 1.00 0.0019
Petal.Width     4  0.97 -0.05 0.94 0.0634

                 PC1  PC2
SS loadings     2.89 0.84
Proportion Var  0.72 0.21
Cumulative Var  0.72 0.93
Cum. factor Var 0.78 1.00

Mean item complexity =  1.2
Test of the hypothesis that 2 components are sufficient.

The root mean square of the residuals (RMSR) is  0.02 
 with the empirical chi square  0.53  with prob &lt;  NA 

Fit based upon off diagonal values = 1</code></pre>
<p>The function’s output matches our calculations. <strong>H2</strong> and <strong>U2</strong> are the *<strong>communality</strong> and <strong>specific variance</strong>, respectively, of the <strong>standardized loadings</strong> obtained from the <strong>correlation matrix</strong> <span class="math inline">\(\mathbf{R}\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris.FA.PC[[<span class="st">&quot;communality&quot;</span>]]</code></pre></div>
<pre><code>Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
   0.6568270    0.1595832    3.1103354    0.5441668 </code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris.FA.PC[[<span class="st">&quot;loadings&quot;</span>]]</code></pre></div>
<pre><code>
Loadings:
             PC1    PC2   
Sepal.Length  0.743  0.323
Sepal.Width  -0.174  0.360
Petal.Length  1.762       
Petal.Width   0.737       

                 PC1   PC2
SS loadings    4.228 0.243
Proportion Var 1.057 0.061
Cumulative Var 1.057 1.118</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">iris.FA.PC[[<span class="st">&quot;Vaccounted&quot;</span>]]</code></pre></div>
<pre><code>                            PC1        PC2
SS loadings           4.2282417 0.24267075
Proportion Var        0.9246187 0.05306648
Cumulative Var        0.9246187 0.97768521
Proportion Explained  0.9457223 0.05427768
Cumulative Proportion 0.9457223 1.00000000</code></pre>
<p><code>Proportion Var</code> is the proportion of total variance due to the column of <span class="math inline">\(\hat{\mathbf{L}}\)</span>, i.e the common factor.</p>
<p><code>Proportion Explained</code> is the variance due to one common factor devided by variance due to all common factors.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="4">
<li id="fn4"><p>It’s reasonable since the last few <span class="math inline">\(\lambda_i\)</span>’s have small values, hence dropping them doesn’t largely affects the total variance of <span class="math inline">\(\mathbf{S}\)</span>.<a href="FA.html#fnref4">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="PCA.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/liao961120/MVA.github.io/edit/master/FA.Rmd",
"text": "Edit"
},
"download": ["Notes.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
