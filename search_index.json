[
["index.html", "R Notes for Multivariate Analysis About Acknowledgement License", " R Notes for Multivariate Analysis Yongfu, Liao 2019-02-13 About This is a very simplified book about Multivariate Analysis in R. It is written as a note to facilitate my learning of Multivariate Analysis at NTU, Spring, 2018. Feel free to edit (by clicking on the nav bar) if you see any error. Also, forking, modifying, and redistributing are welcomed. Acknowledgement I thank Aaron Schlegel for letting me modify his post on factor analysis with the principle component method. Chapter 3 and Appendix A of this book is largely based on this post. License R Notes for Multivariate Analysis by Yongfu, Liao is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.Based on a work at https://github.com/liao961120/MVA. "],
["mvnorm.html", "Chapter 1 Multivariate Normal Distribution &amp; Covariance Matrix 1.1 Bivariate Normal Contour Map 1.2 Multivariate Normal Functions", " Chapter 1 Multivariate Normal Distribution &amp; Covariance Matrix library(dplyr) library(latex2exp) library(ggplot2) theme &lt;- theme(axis.text.x = element_text(size = 7, face = &quot;plain&quot;, angle = 30), axis.text.y = element_text(size = 7, face = &quot;plain&quot;), axis.title.x = element_text(size = 9, face = &quot;bold&quot;), axis.title.y = element_text(size = 9, face = &quot;bold&quot;)) 1.1 Bivariate Normal Contour Map 1.1.1 ellipse() ellipse() from ellipse (Murdoch and Chow 2018) is used to generate ellipse data based on a correlation/covariance matrix. ellipse(x, scale, centre, level, npoints = 1000) x: a single number, correlation of the two variables. scale: vector, standard deviation of the two variables. centre: vector, center of the ellipse, i.e. the mean vector of the bivariate normal distribution. level: a single number, the contour probability. npoints: number of points used to draw the contour. ellipse returns a matrix with dim(npoints \\(\\times\\) 2), which can be used to plot contour. 1.1.2 Data Generation The for loop below is used to generate a data frame with 3 columns(variables): Column 1: First variable of bivariate normal function (\\(x_1\\)) Column 2: Second variable of bivariate normal function (\\(x_2\\)) Column 3: The contour that \\(x_1\\) &amp; \\(x_2\\) on the same row belongs to. library(ellipse) All_contours &lt;- c(NA, NA, NA) ## Set empty start for appending ## for (i in 1:5) { level &lt;- 0.1*i ## Set Contour prob., prob. of obs within contour ## ell_data &lt;-ellipse(-0.8, c(sqrt(2), 1), centre = c(1, 3), level = level, npoints = 800+(i-1)^3) ## npoints: bigger contours with more points ## class &lt;- rep(paste(level*100, &quot;% Contour&quot;, sep=&quot;&quot;), nrow(ell_data)) ## Assign contour class ## ell_data &lt;- as.data.frame(ell_data) ## Change to data.frame BEFORE cbind, ## ## or coersion happens ## ell_data &lt;- cbind(ell_data, class) All_contours &lt;- rbind(All_contours, ell_data) } All_contours &lt;- All_contours[-1,] ## Remove the empty start ## 1.1.3 Plotting ggplot(data = All_contours) + geom_point(aes(x = x, y = y, color = class), size = 0.1) + scale_colour_grey(start = 0.7, end = 0.3) + ## Use gray scales instead of colored default ## labs(color = &quot;Contours&quot;, title = &quot;Contour Plot&quot;, x = TeX(&quot;$x_1$&quot;), y = TeX(&quot;$x_2$&quot;) ) 1.2 Multivariate Normal Functions 1.2.1 Generate density f(x) library(mvtnorm) mu &lt;- c(1, 3) # mean vector Sigma &lt;- matrix(c(2, -0.8*sqrt(2), -0.8*sqrt(2), 1), nrow = 2) # covariance matrix dmvnorm(x = c(2, 5), mean = mu, sigma = Sigma) [1] 1.562995e-05 x: Vector x in f(x), all variables of the multivariate normal distribution. mean: Mean vector(center of ellipse) of the multivariate normal distribution. sigma: Covariance matrix of the multivariate normal distribution. dmvnorm returns f(x), the range of the multivariate normal function. For example, dmvnorm(x = c(2, 5), mean = mu, sigma = Sigma) returns the value f(\\(x_1=2\\), \\(x_2=5\\)) of the multivariate normal distribution specified by mean vector, mu, and covariance matrix, Sigma. 1.2.1.1 Example: Densities of a Contour data &lt;- All_contours %&gt;% filter(class == &quot;50% Contour&quot;) dmvnorm(x = data[1, 1:2], mean = mu, sigma = Sigma)[[1]] [1] 0.09378295 dmvnorm(x = data[4, 1:2], mean = mu, sigma = Sigma)[[1]] [1] 0.09378295 The retured values are the same(very close), since they are on the same contour. See the section above for more details. 1.2.2 Covariance Matrix Generater covariance and correlation Matricies: # Function to check whether package is installed is.installed &lt;- function(mypkg){ is.element(mypkg, installed.packages()[,1]) } # check if package &quot;hydroGOF&quot; is installed if (!is.installed(&quot;mat2tex&quot;)){ library(devtools) install_github(&quot;markheckmann/mat2tex&quot;) } checking for file ‘/tmp/RtmpideFlh/remotes4865181a063/markheckmann-mat2tex-d6ba4c1/DESCRIPTION’ ... ✔ checking for file ‘/tmp/RtmpideFlh/remotes4865181a063/markheckmann-mat2tex-d6ba4c1/DESCRIPTION’ ─ preparing ‘mat2tex’: checking DESCRIPTION meta-information ... ✔ checking DESCRIPTION meta-information ─ checking for LF line-endings in source and make files and shell scripts ─ checking for empty or unneeded directories ─ building ‘mat2tex_0.1.9002.tar.gz’ library(mat2tex) cov.mt &lt;- cov(iris[,1:4]) ## Cov Matrix of variable 1~4 cor.mt &lt;- cor(iris[,1:4]) ## Cor Matrix of variable 1~4 Covariance matrix \\(= \\begin{pmatrix} 0.69 &amp; -0.04 &amp; 1.27 &amp; 0.52 \\\\ -0.04 &amp; 0.19 &amp; -0.33 &amp; -0.12 \\\\ 1.27 &amp; -0.33 &amp; 3.12 &amp; 1.30 \\\\ 0.52 &amp; -0.12 &amp; 1.30 &amp; 0.58 \\\\ \\end{pmatrix}\\) Correlation matrix \\(= \\begin{pmatrix} 1.00 &amp; -0.12 &amp; 0.87 &amp; 0.82 \\\\ -0.12 &amp; 1.00 &amp; -0.43 &amp; -0.37 \\\\ 0.87 &amp; -0.43 &amp; 1.00 &amp; 0.96 \\\\ 0.82 &amp; -0.37 &amp; 0.96 &amp; 1.00 \\\\ \\end{pmatrix}\\) Package Used "],
["PCA.html", "Chapter 2 Principle Component Analysis 2.1 Workflow of PCA 2.2 Conversion Between Correlation &amp; Covaraince Matrices 2.3 Scree Plot 2.4 Q-Q Plot", " Chapter 2 Principle Component Analysis 2.1 Workflow of PCA 2.1.1 Conceptual 2.1.2 Computational (with R) Note: sdev of prcomp() are Standard Deviations. To get the eigenvalues of the covariance (correlation) matrix, or equivalently, variances of the principle components, you need to square sdev. 2.2 Conversion Between Correlation &amp; Covaraince Matrices 2.2.1 prcomp() The function prcomp() in base R stats package performs principle component analysis to input data.frame(with observations as rows and variables as columns), but it returns neither covariance nor correlation matrix. You can compute them directly by passing data.frame to cor() and cov() directly in R without any additional package. prcomp() vs. princomp() There is another function, princomp(), in stats that performs PCA. This function is based on spectral decomposition1 while prcomp() is based on SVD2. SVD has greater numeric accuracy, so prcomp() is preferred. 2.2.2 Covariance to Correlation Sometimes there is no raw data but only covariance or correlation matrix, and you may want to convert one to another. This can be done by using simple matrix multiplication, based on the fact that \\[\\mathbf{R} = diag(\\mathbf{S})^{\\frac{-1}{2}} ~ \\mathbf{S} ~ diag(\\mathbf{S})^{\\frac{-1}{2}} \\tag{2.1}\\] , where \\(\\mathbf{R}\\) is the correlation matrix, \\(\\mathbf{S}\\) is the covariance matrix, and \\(diag(\\mathbf{S})\\) is the diagonal matrix composed of diagonal elements of \\(\\mathbf{S}\\). 2.2.3 eigen() After obtaining the covariance or correlation matrix, direct computation of eigenvalue and eigenvectors is straightforward: pass the matrix to base R eigen() function. cov(iris[,1:3]) %&gt;% eigen() eigen() decomposition $values [1] 3.69111979 0.24137727 0.05945372 $vectors [,1] [,2] [,3] [1,] 0.38983343 0.6392233 -0.6628903 [2,] -0.09100801 0.7430587 0.6630093 [3,] 0.91637735 -0.1981349 0.3478435 2.3 Scree Plot Scree plot is an important tool for determining the importance of principle components. Although the logic of plotting scree plots is easy, it may be quite annoying for repeating the code every time. 2.3.1 screeplot() from Base R There is a ready-written function for scree plot in stats package, but the output is terrible: prcomp(iris[,1:3]) %&gt;% screeplot(type=&quot;lines&quot;) 2.3.2 fviz_eig() from factoextra For a better-looking scree plot function, I recommend fviz_eig() from factoextra (Kassambara and Mundt 2017). fviz_eig() has better looking outputs and more customizable plotting parameters, and since it is based on ggplot2, you can actually enhance it with the ggplot2 syntax: +. library(factoextra) prcomp(iris[,1:3]) %&gt;% fviz_eig() prcomp(iris[,1:3]) %&gt;% fviz_eig(choice = &quot;eigenvalue&quot;, # y as eigenvalue geom = &quot;line&quot;, addlabels = T) + scale_y_continuous(limits = c(0, 5)) 2.3.3 Customized Function I have OCD with plotting, so not completely satisfied with factoextra::fviz_eig(). So I created my own scree_plot() by building on fviz_eig()3, which supports double y-axis: one showing eigenvalue, the other proportion of total variance explained. 2.4 Q-Q Plot Q-Q plots are for checking the normality assumuption and are also useful for detecting outlyers. Principle components are linear combinations of the original variables, so if the original variables come from a multivariate normal distribution, principle components are expected to have normal distributions. 2.4.1 qqnorm() from Base R There is also a base R qqnorm() function, which plots sample quantiles against theoretical quantiles obtain from the standard normal distribution. prcomp(iris[1:60, 1:3])[[&quot;x&quot;]][,1] %&gt;% qqnorm() prcomp(data.frame)[[&quot;x&quot;]] returns the principle component scores, i.e. data that are rotated or weighted by the elements of the eigenvectors. prcomp(data.frame)[[&quot;x&quot;]][,1] subsets the first column of the principle component scores, which is the scores of the First principle component, i.e. data weighted according to elements of the first (corresponding to the largest eigenvalue) eigenvector. 2.4.2 Self-defined Function qqnorm() is pretty good but lacks one important feature: labeling points on the Q-Q plot so that identification of the points is possible. So I wrote my own function QQplot, which labels every point on the graph: QQplot &lt;- function(x, ID=&quot;none&quot;, theme=NULL, color=&quot;red&quot;, text=TRUE, text_adj= c(hjust=-0.1, vjust=0, size=3)) { library(dplyr) library(ggplot2) x &lt;- as_data_frame(x) n &lt;- nrow(x) quantiles &lt;- qnorm(p=seq(0.5/n, 1-0.5/n, 1/n)) if (ID == &quot;none&quot;) { # assign ID if not passed ID &lt;- as.character(1:n) } else { ID &lt;- as_data_frame(ID) ID &lt;- as.character(ID[[colnames(ID)]]) } if (text == TRUE) { text &lt;- geom_text(aes(label=ID), hjust=text_adj[1], vjust=text_adj[2], size = text_adj[3]) } else {text &lt;- NULL} data &lt;- cbind(ID, x) colnames(data) &lt;- c(&quot;ID&quot;, &quot;x&quot;) data &lt;- data %&gt;% arrange(x) %&gt;% mutate(quantile=quantiles) pl &lt;- ggplot(data, aes(x=quantiles, y=x))+ geom_point(color=color)+ text + theme + labs(x=&quot;Theoretical Quantile&quot;, y=&quot;x&quot;, title=&quot;Q-Q Plot&quot;) pl } prcomp(iris[1:60, 1:3])[[&quot;x&quot;]][,1] %&gt;% QQplot() Package Used "],
["FA.html", "Chapter 3 Factor Analysis 3.1 The Concept of Factor Analysis 3.2 Principal Component Method 3.3 Factor Analysis with the psych Package", " Chapter 3 Factor Analysis Factor analysis is a technique that represents the variables of a dataset \\(X_1, X_2, \\cdots, X_p\\) (or \\(\\bm{X}_{p \\times 1}\\)) as linearly related to some fewer unobservable variables called factors, denoted \\(F_1, F_2, \\cdots, F_m\\) (or \\(\\bm{F}_{m \\times 1}\\)). The factors are representative of latent variables underlying the original variables, which is hypothetical as they cannot be measured or observed. Notations: Capital Letter: Random Variable Lower case Letter: Observation of a random variable Bold: Vector or Matrix Normal: Single Value 3.1 The Concept of Factor Analysis The Orthogonal Factor Model posits that each variable, \\(X_i\\) is a combination of the underlying latent variables, \\(F_1, F_2, \\cdots, F_m\\). For the variables in any of the observation vectors in a sample, the model is defined as: \\[\\begin{equation} \\begin{split} X_1 - \\mu_1 = \\ell_{11} F_1 + \\ell_{12} F_2 + &amp;\\cdots + \\ell_{1m} F_m + \\varepsilon_1 \\\\ X_2 - \\mu_2 = \\ell_{21} F_1 + \\ell_{22} F_2 + &amp;\\cdots + \\ell_{2m} F_m + \\varepsilon_2 \\\\ &amp; ~~ \\vdots \\\\ X_p - \\mu_p = \\ell_{p1} F_1 + \\ell_{p2} F_2 + &amp;\\cdots + \\ell_{pm} F_m + \\varepsilon_p \\end{split} \\tag{3.1} \\end{equation}\\] , or \\[ \\bm{X}_{p \\times 1} - \\bm{\\mu}_{p \\times 1} = \\bm{L}_{p \\times m} \\bm{F}_{m \\times 1} + \\bm{\\varepsilon}_{p \\times 1} \\tag{3.2} \\] , where \\(\\bm{\\mu}\\) is the mean vector and \\(\\bm{\\varepsilon}\\) is a random error term to show the relationship between the factors is not exact. There are several assumptions that must be made regarding the relationships of the factor model described above. Factors are independent of each other. \\(E(F_j) = 0\\), \\(Var(F_j) = 1\\) \\(Cov(F_j, F_k) = 0\\) where \\(j \\neq k\\) The error terms \\(\\varepsilon_i\\) are independent of each other. \\(E(\\varepsilon) = 0\\), \\(Var(\\varepsilon_i) = \\psi_i\\) \\(Cov(\\varepsilon_i, \\varepsilon_j) = 0\\). \\(\\varepsilon_i\\) and \\(F_j\\) are independent: \\(Cov(\\varepsilon_i, F_j) = 0\\). Note the assumption \\(Cov(\\varepsilon_i, \\varepsilon_j) = 0\\) implies the factors represent all correlations among the variables \\(X_i\\)’s. 3.1.1 Factoring the Covariance Matrix With the assumptions above, \\[ \\begin{equation} \\begin{split} &amp; \\bm{\\Sigma}_{p \\times p} = Cov(\\bm{X}_{p \\times p}) = \\bm{L L^T}_{p \\times p} + \\bm{\\Psi}_{p \\times p} \\\\ \\\\ , where ~~ &amp; \\bm{L}_{p \\times m} = \\begin{pmatrix} \\ell_{11} &amp; \\ell_{12} &amp; \\cdots &amp; \\ell_{1m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\ell_{p1} &amp; \\ell_{p2} &amp; \\cdots &amp; \\ell_{pm} \\end{pmatrix}, \\\\ &amp; \\bm{\\Psi}_{p \\times p} = \\begin{pmatrix} \\psi_{1} &amp; &amp; 0\\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; \\psi_{p} \\end{pmatrix} \\end{split} \\tag{3.3} \\end{equation} \\] \\(Var(X_i) = \\sigma_{ii} = \\ell^2_{i1} + \\ell^2_{i2} + \\cdots + \\ell^2_{im} + \\psi_i\\) \\(Cov(X_i, X_j) = \\sigma_{ij} = \\ell_{i1}\\ell_{j1} + \\ell_{i2}\\ell_{j2} + \\cdots + \\ell_{im}\\ell_{jm}\\) \\(Cov(\\bm{X}, \\bm{F}) = \\bm{L}\\), i.e. \\(\\ell_{ij} = Cov(X_i, F_j)\\) We therefore have a partitioning of the variance of the observation vector \\(X_i\\) into a component due to the common factors \\(h_i^2\\) and a component due to specific variance: \\[ \\begin{equation} \\begin{split} Var(X_i) &amp; = (\\ell^2_{i1} + \\ell^2_{i2} + \\cdots + \\ell^2_{im}) + \\psi_i \\\\ &amp; = h^2_i + \\psi_i \\end{split} \\tag{3.4} \\end{equation} \\] 3.2 Principal Component Method There are several methods for estimating the factor loadings and communalities, including the principal component method, principal factor method, the iterated principal factor method and maximum likelihood method. The approach of the principal component method is to calculate the sample covariance matrix \\(\\bm{S}\\) from a sample of data and then find an estimator, denoted \\(\\hat{\\ell}\\) that can be used to factor \\(\\bm{S}\\). By orthogonal diagonalizing \\(\\bm{S}\\): \\[ \\begin{equation} \\begin{split} &amp;\\bm{S} = PDP^T \\\\ \\\\ , ~ where \\ &amp; D = \\begin{pmatrix} \\lambda_1 &amp; &amp; \\\\ &amp; \\ddots &amp; \\\\ &amp; &amp; \\lambda_p \\end{pmatrix}_{p \\times p} , \\\\ &amp; P = \\begin{bmatrix} e_1 \\ \\vdots &amp; \\cdots &amp; \\vdots \\ e_p \\end{bmatrix}_{p \\times p} \\end{split} \\tag{3.5} \\end{equation} \\] \\(D\\) is a diagonal matrix with the diagonal entries, \\(\\lambda_1 &gt; \\lambda_2 &gt; \\cdots &gt; \\lambda_p\\) equaling the eigenvalues of \\(\\bm{S}\\). \\(P\\) is an orthogonal matrix with columns of eigenvectors of \\(\\bm{S}\\) corresponding to \\(D\\). By factoring the diagonal matrix \\(D\\), \\(~D = D^{1/2} ~ D^{1/2}\\), since all \\(\\lambda_i \\geq 0\\): \\[ \\begin{equation} \\begin{split} \\bm{S} &amp;= PDP^T = (P D^{1/2})(D^{1/2} P^T) \\\\ &amp; = (PD^{1/2})(PD^{1/2})^T \\\\ &amp; = \\bm{LL^T} \\end{split} \\tag{3.6} \\end{equation} \\] Since we are interested in finding \\(m\\)(\\(&lt;p\\)) factors in the data, we want to find \\(\\bm{L}_{p \\times m}\\). By dropping the last \\((p-m)\\) terms in \\(D\\)4, the loading matrix then becomes \\(\\bm{L}_{p \\times m} = P_{p \\times m} ~ D_{m \\times m}\\). And, \\[ \\begin{equation} \\begin{split} &amp; \\bm{S} \\approx \\bm{L}_{p \\times m} \\bm{L}^T_{m \\times p} + \\bm{\\Psi}_{p \\times p} \\\\ \\\\ , where ~~ &amp; \\bm{\\Psi} = diag(\\bm{S} - \\bm{LL}^T) \\\\ &amp; ~~~ = \\begin{pmatrix} s_{11} - h_1^2 &amp; &amp; 0 \\\\ &amp; \\ddots &amp; \\\\ 0 &amp; &amp; s_{pp} - h_p^2 \\end{pmatrix} \\end{split} \\tag{3.7} \\end{equation} \\] The number \\(m\\) could be determined by a scree plot. 3.3 Factor Analysis with the psych Package The psych package has many functions available for performing factor analysis. library(psych) The principal() function performs factor analysis with the principal component method as explained above. The rotation is set to none for now as we have not yet done any rotation of the factors. The covar argument is set to TRUE so the function factors the covariance matrix \\(\\bm{S}\\) of the data as we did above. library(psych) iris.FA.PC &lt;- principal(iris[,-5], nfactors = 2, rotate = &#39;none&#39;, covar = TRUE) iris.FA.PC Principal Components Analysis Call: principal(r = iris[, -5], nfactors = 2, rotate = &quot;none&quot;, covar = TRUE) Unstandardized loadings (pattern matrix) based upon covariance matrix PC1 PC2 h2 u2 H2 U2 Sepal.Length 0.74 0.32 0.66 0.0289 0.96 0.0421 Sepal.Width -0.17 0.36 0.16 0.0304 0.84 0.1600 Petal.Length 1.76 -0.09 3.11 0.0059 1.00 0.0019 Petal.Width 0.74 -0.04 0.54 0.0368 0.94 0.0634 PC1 PC2 SS loadings 4.23 0.24 Proportion Var 0.92 0.05 Cumulative Var 0.92 0.98 Proportion Explained 0.95 0.05 Cumulative Proportion 0.95 1.00 Standardized loadings (pattern matrix) item PC1 PC2 h2 u2 Sepal.Length 1 0.90 0.39 0.96 0.0421 Sepal.Width 2 -0.40 0.83 0.84 0.1600 Petal.Length 3 1.00 -0.05 1.00 0.0019 Petal.Width 4 0.97 -0.05 0.94 0.0634 PC1 PC2 SS loadings 2.89 0.84 Proportion Var 0.72 0.21 Cumulative Var 0.72 0.93 Cum. factor Var 0.78 1.00 Mean item complexity = 1.2 Test of the hypothesis that 2 components are sufficient. The root mean square of the residuals (RMSR) is 0.02 with the empirical chi square 0.53 with prob &lt; NA Fit based upon off diagonal values = 1 The function’s output matches our calculations. H2 and U2 are the *communality and specific variance, respectively, of the standardized loadings obtained from the correlation matrix \\(\\bm{R}\\). iris.FA.PC[[&quot;communality&quot;]] Sepal.Length Sepal.Width Petal.Length Petal.Width 0.6568270 0.1595832 3.1103354 0.5441668 iris.FA.PC[[&quot;loadings&quot;]] Loadings: PC1 PC2 Sepal.Length 0.743 0.323 Sepal.Width -0.174 0.360 Petal.Length 1.762 Petal.Width 0.737 PC1 PC2 SS loadings 4.228 0.243 Proportion Var 1.057 0.061 Cumulative Var 1.057 1.118 iris.FA.PC[[&quot;Vaccounted&quot;]] PC1 PC2 SS loadings 4.2282417 0.24267075 Proportion Var 0.9246187 0.05306648 Cumulative Var 0.9246187 0.97768521 Proportion Explained 0.9457223 0.05427768 Cumulative Proportion 0.9457223 1.00000000 Proportion Var is the proportion of total variance due to the column of \\(\\hat{\\bm{L}}\\), i.e the common factor. Proportion Explained is the variance due to one common factor devided by variance due to all common factors. It’s reasonable since the last few \\(\\lambda_i\\)’s have small values, hence dropping them doesn’t largely affects the total variance of \\(\\bm{S}\\).↩ "],
["principle-component-method-explained-in-r.html", "A Principle Component Method Explained (in R) A.1 \\(\\lambda_i\\) &amp; \\(e_i\\) of \\(\\mathbf{S}\\) A.2 Construction of \\(\\hat{\\mathbf{L}}\\) A.3 Variances: Communalities, &amp; Specific Variances", " A Principle Component Method Explained (in R) The following example demonstrates factor analysis using the covariance matrix with the iris data set in R. Instead of directly using psych::priciple(), a step-by-step approach is used. Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa A.1 \\(\\lambda_i\\) &amp; \\(e_i\\) of \\(\\mathbf{S}\\) Find the covariance matrix \\(\\mathbf{S}\\) with the cov() function. S &lt;- cov(iris[,-5]) S %&gt;% kable(format=&quot;html&quot;, align=&quot;c&quot;, digits = 3) %&gt;% kable_styling(bootstrap_options = c(&quot;striped&quot;, &quot;condensed&quot;)) %&gt;% scroll_box(width = &quot;100%&quot;) Sepal.Length Sepal.Width Petal.Length Petal.Width Sepal.Length 0.686 -0.042 1.274 0.516 Sepal.Width -0.042 0.190 -0.330 -0.122 Petal.Length 1.274 -0.330 3.116 1.296 Petal.Width 0.516 -0.122 1.296 0.581 The eigenvalues and eigenvectors are then computed from the covariance matrix with the eigen() function. S.eigen &lt;- eigen(S) S.eigen eigen() decomposition $values [1] 4.22824171 0.24267075 0.07820950 0.02383509 $vectors [,1] [,2] [,3] [,4] [1,] 0.36138659 -0.65658877 -0.58202985 0.3154872 [2,] -0.08452251 -0.73016143 0.59791083 -0.3197231 [3,] 0.85667061 0.17337266 0.07623608 -0.4798390 [4,] 0.35828920 0.07548102 0.54583143 0.7536574 A.2 Construction of \\(\\hat{\\mathbf{L}}\\) Before proceeding with factoring \\(\\mathbf{S}\\) into \\(PDP^T\\), the number of factors \\(m\\) must be selected. The last two eigenvalues of \\(\\mathbf{S}\\) are practically \\(0\\), so \\(m = 2\\) is likely a good choice. With \\(m = 2\\) factors, construct the \\(P\\) and \\(D\\) matrices from the covariance matrix with the largest two eigenvalues and the corresponding eigenvectors. P &lt;- S.eigen$vectors[,1:2] %&gt;% as.matrix() D &lt;- diag(S.eigen$values[1:2], nrow = dim(P)[2], ncol = dim(P)[2]) \\(\\hat{\\mathbf{L}}\\) is then found from the \\(P\\) and \\(D\\) matrices as in \\(\\hat{\\mathbf{L}} = PD^{1/2}\\) S.loadings &lt;- P %*% sqrt(D) S.loadings %&gt;% round(3) [,1] [,2] [1,] 0.743 -0.323 [2,] -0.174 -0.360 [3,] 1.762 0.085 [4,] 0.737 0.037 Which are the unrotated factor loadings. We can see where the term principal component method is derived from as the columns of \\(\\hat{\\mathbf{L}}\\) are proportional to the eigenvectors of \\(\\mathbf{S}\\), which are also equal to the corresponding coefficient of the principal components. # Perform PCA and take the resulting first two PCs prcomp(iris[,-5])$rotation[,1:2] %&gt;% round(3) PC1 PC2 Sepal.Length 0.361 -0.657 Sepal.Width -0.085 -0.730 Petal.Length 0.857 0.173 Petal.Width 0.358 0.075 # eigenvector of S from iris S.eigen$vectors[,1:2] %&gt;% round(3) [,1] [,2] [1,] 0.361 -0.657 [2,] -0.085 -0.730 [3,] 0.857 0.173 [4,] 0.358 0.075 A.3 Variances: Communalities, &amp; Specific Variances The communality \\(h^2_i\\), as noted previously, is the sum of squares of the row of \\(\\hat{\\mathbf{L}}\\). \\[ \\hat{h}^2_i = \\sum^m_{j=1} \\hat{\\ell}^2_{ij} \\] S.h2 &lt;- S.loadings^2 %&gt;% rowSums() S.h2 %&gt;% round(3) [1] 0.657 0.160 3.110 0.544 The sum of squares of the columns of \\(\\hat{\\mathbf{L}}\\) are the respective eigenvalues of \\(\\mathbf{S}\\), since \\[ \\sqrt{\\hat{\\lambda_i}} \\hat{e_i} = \\begin{pmatrix} \\ell_{i1} \\\\ \\ell_{i2} \\\\ \\vdots \\\\ \\ell_{ip} \\\\ \\end{pmatrix}_{p \\times 1}, ~ (\\sqrt{\\hat{\\lambda_i}} \\hat{e_i})^T (\\sqrt{\\hat{\\lambda_i}} \\hat{e_i}) = \\hat{\\lambda_i} \\] colSums(S.loadings^2) %&gt;% round(3) [1] 4.228 0.243 S.eigen$values[1:2] %&gt;% round(3) [1] 4.228 0.243 A.3.1 Specific variance \\(\\psi_i\\) The specific variance, \\(\\psi_i\\), is a component unique to the particular variable and is found by subtracting the diagonal of \\(\\mathbf{S}\\) by the respective communality \\(\\hat{h}^2_i\\): \\[ \\psi_i = s_{ii} - \\hat{h}^2_i \\] diag(S) - S.h2 %&gt;% round(4) Sepal.Length Sepal.Width Petal.Length Petal.Width 0.028893512 0.030379418 0.005977852 0.036806264 A.3.2 Proportion of variance due to Factors The proportions of total variance of \\(\\mathbf{S}\\) due to common factor 1 and 2, respectively, are found by dividing the sum of squares of the columns of \\(\\hat{\\mathbf{L}}\\) by \\(tr(\\mathbf{S}) = \\Sigma s_{ii} = \\Sigma \\lambda_i\\). var_F1 &lt;- colSums(S.loadings^2)[1] var_F2 &lt;- colSums(S.loadings^2)[2] cbind(var_F1 / sum(S.eigen$values), var_F2 / sum(S.eigen$values)) %&gt;% round(3) [,1] [,2] [1,] 0.925 0.053 "],
["references.html", "Package Used", " Package Used "]
]
